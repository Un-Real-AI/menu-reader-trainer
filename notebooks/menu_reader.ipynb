{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f900e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c4dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MenuDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root,path):\n",
    "        self.root = root\n",
    "        self.path=path\n",
    "        self._transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted([img for img in os.listdir(os.path.join(root, path+\"/images\")) if \n",
    "                                 ('.jpg' in img) or ('jpeg'in img) or('png'in img)]))\n",
    "        self.masks = list(sorted([mask for mask in os.listdir(os.path.join(root, path+\"/labels\")) if '.txt' in mask]))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, self.path+\"/images\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, self.path+\"/labels\", self.masks[idx])\n",
    "        img = Image.open(img_path)\n",
    "        # get bounding box coordinates for each mask\n",
    "        objs=np.loadtxt(mask_path)\n",
    "        if(objs.ndim==1):\n",
    "            objs=np.array([objs])\n",
    "        num_objs = np.shape(objs)[0]\n",
    "        w,h=img.size\n",
    "        boxes = objs[:,1:]\n",
    "        boxes[:,0]=boxes[:,0]*w\n",
    "        boxes[:,1]=boxes[:,1]*h\n",
    "        boxes[:,2]=boxes[:,2]*w\n",
    "        boxes[:,3]=boxes[:,3]*h\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torchvision.ops.box_convert(torch.tensor(boxes),'cxcywh','xyxy')\n",
    "        # there is only one class\n",
    "        labels = torch.as_tensor([int(obj[0])+1 for obj in objs], dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img),target\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca51427",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MenuDataset('me.v3-90-10.yolov5pytorch','train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21d9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "num_classes = 5 # background, item,description,title and price\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ebdf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True)\n",
    "images,targets = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ccc6b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "images = [images]\n",
    "targets = [targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(images)  # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae245f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        TT=transforms.ToPILImage()\n",
    "        img = TT(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdc17119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model):\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 5\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = MenuDataset('me.v3-90-10.yolov5pytorch','train/')\n",
    "    dataset_test = MenuDataset('me.v3-90-10.yolov5pytorch','test/')\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "    \n",
    "    #helper functions \n",
    "    tensor_to_int=transforms.ConvertImageDtype(torch.uint8)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses_list=[]\n",
    "        for i in tqdm(range(dataset.__len__())):\n",
    "            images,targets = dataset.__getitem__(i)\n",
    "            images = [images]\n",
    "            targets = [targets]\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            loss_dict = model(images,targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            losses_list.append(loss_value)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            if i%100==0:\n",
    "                print(\"Current Loss: \",sum(losses_list)/len(losses_list))\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        model.eval()\n",
    "        predictions_imgs=[]\n",
    "        \n",
    "        for i in tqdm(range(dataset_test.__len__())):\n",
    "            images,targets = dataset_test.__getitem__(i)\n",
    "            images = [images]\n",
    "            targets = [targets]\n",
    "            outputs = model(images)\n",
    "            img=tensor_to_int(images[0])\n",
    "            predictions_imgs.append(draw_bounding_boxes(img, outputs[0]['boxes'], colors=\"yellow\",width=6))\n",
    "            \n",
    "    show(predictions_imgs)\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88acd0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/113 [00:03<06:50,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.5639121501298073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:40<00:40,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  1.0501482845256742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:20<00:00,  3.37s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:20,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.6637884973818375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:40<00:40,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  1.0638431913166317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:20<00:00,  3.37s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:25,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.6958190830093316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:41<00:40,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  1.0457621524697427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:21<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:20<00:00,  1.74s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:33,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.6143025369632947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:41<00:40,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  1.012292560323083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:22<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:20<00:00,  1.69s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:36,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.5302072858124585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:39<00:40,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.9590625235276296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:19<00:00,  3.36s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:19<00:00,  1.66s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:12,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.5294565313221474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:38<00:40,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.9423984353989815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:17<00:00,  3.34s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:19<00:00,  1.65s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:13,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.5461807269014676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:39<00:39,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.9229582323865171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:18<00:00,  3.35s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:19<00:00,  1.66s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:25,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.4961875679206373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:38<00:40,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.9223913413171746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:18<00:00,  3.35s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:19<00:00,  1.65s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<06:08,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.5282086412172132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:36<00:39,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.9188659718901637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:16<00:00,  3.33s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:19<00:00,  1.63s/it]\n",
      "  1%|▍                                          | 1/113 [00:03<05:56,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.5331881785668027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 101/113 [05:36<00:39,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss:  0.9192756988472177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 113/113 [06:15<00:00,  3.33s/it]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:19<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "main(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "765187f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "dataset_test = MenuDataset('me.v3-90-10.yolov5pytorch','test/')\n",
    "dict_labels={1:'description',2:'item',3:'price',4:'tittle'}\n",
    "colors_dict={1:\"yellow\",2:\"red\",3:\"green\",4:\"blue\"}\n",
    "\n",
    "img,target=dataset_test.__getitem__(8)\n",
    "model.eval()\n",
    "result=model([img])\n",
    "indexes=[item.item() for item in result[0]['scores']>0.5]\n",
    "tt=transforms.ConvertImageDtype(torch.uint8)\n",
    "img=tt(img)\n",
    "        \n",
    "drawn_boxes = draw_bounding_boxes(img, result[0]['boxes'][indexes],\n",
    "                    labels=[dict_labels[item.item()] for item in result[0][\"labels\"][indexes]], \n",
    "                    colors=[colors_dict[item.item()] for item in result[0][\"labels\"][indexes]],width=6)\n",
    "show(drawn_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dff27ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9152, 0.8686, 0.8670, 0.8524, 0.8392, 0.8374, 0.8340, 0.8325, 0.8238,\n",
       "        0.8184, 0.8013, 0.8012, 0.8002, 0.7981, 0.7705, 0.7699, 0.7550, 0.7534,\n",
       "        0.7502, 0.7325, 0.7252, 0.7185, 0.7084, 0.7012, 0.6992, 0.6940, 0.6904,\n",
       "        0.6884, 0.6851, 0.6844, 0.6627, 0.6523, 0.6490, 0.6411, 0.6395, 0.6276,\n",
       "        0.6118, 0.5943, 0.5848, 0.5751, 0.5664, 0.5640, 0.5608, 0.5564, 0.5560,\n",
       "        0.5462, 0.5230, 0.5219, 0.5142, 0.5125, 0.5096, 0.5075, 0.5059, 0.5051,\n",
       "        0.4847, 0.4829, 0.4733, 0.4637, 0.4583, 0.4562, 0.4548, 0.4543, 0.4537,\n",
       "        0.4492, 0.4358, 0.4140, 0.4083, 0.3944, 0.3902, 0.3809, 0.3806, 0.3775,\n",
       "        0.3656, 0.3629, 0.3412, 0.3405, 0.3272, 0.3269, 0.3217, 0.3134, 0.3038,\n",
       "        0.2951, 0.2944, 0.2717, 0.2680, 0.2614, 0.2528, 0.2510, 0.2395, 0.2335,\n",
       "        0.2320, 0.2278, 0.2276, 0.2220, 0.2146, 0.2131, 0.2129, 0.1956, 0.1939,\n",
       "        0.1910], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5826e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
